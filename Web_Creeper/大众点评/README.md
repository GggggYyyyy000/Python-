# dazhongdianping
python，大众点评的爬虫，突破反爬，获取关于任意店铺的评论和评分之类的。给出破解css加密的方法

## 4-24号更新，给出selenium库的爬虫实现
### by-selenium.py  
由于之前的那两个使用request库的爬虫效率有点低，而且写得比较乱，还存在一个严重的问题：**无法实现展开评论**，所以后来实现了一个新的方法，用selenium进行登录并实现展开评论，然后用bs4库进行解析xml，效率大大提高而且能实现展开评论的全部获取。  
在这里首先是配置好要用的库，要有Chromedriver。在py里面，修改Chromedriver的调用路径，以及driver.get的url参数，还有就是最上面的page（要爬多少页），name（csv文件的命令），在运行的时候需要手机扫码在15秒内登陆大众点评就行，由于设置了time.sleep，所以基本不会被反爬识别出来。  
效率很高，需要更改的变量很少。


**大众点评的评论页面的话目前有两种不同的页面，一种是店铺下的评论（例：http://www.dianping.com/shop/1541380745 ），另外一种是店铺详细的评论（例：http://www.dianping.com/shop/1541380745/review_all/p2 ），两者的区别是后缀不同。**

刚开始弄这个大众点评的评论的时候其实是遇到了很多坑的，这是我第一次遇到有难度的反爬机制，以前的其实只是限制ip啊什么的就比较简单，这次的话用了加密来进行反爬，不知道方法的话就比较头疼。大众点评里面使用的主要是css加密算法，这里的话就不多介绍css加密算法的原理什么的，我们的主要目的是直接获取大众点评的评论代码嘛，所以关于这个css加密是怎么实现的，大家可以参考别的博客进行查询。附上我进行参考的几个链接：  
https://blog.csdn.net/Obgo_6/article/details/100585363  
https://blog.csdn.net/weixin_42512684/article/details/86775357  
https://blog.csdn.net/sinat_32651363/article/details/85123876

爬虫和反爬其实是一个斗智斗勇的过程，所以大众点评的反爬也在不断更新，不过上面说的第一个评论页面（即没有后缀的那个url例子）在查询源代码的时候已经看不到评论了，所以我这里的第一个方法应该是用不了了，当然啦，理论上还是可以爬的。第二个评论页面（有后缀的那个url例子）在这里的话我的这个爬虫目前（2020.4.7）还是有效的，但是过一些日子就不清楚了。

**1. 这里先介绍第一个没有后缀的评论的爬取方法（虽然这个好像被禁掉了……）**
以http://www.dianping.com/shop/1541380745 这个url为例子，打开之后F12分析源代码，里面有一个url是以's3c'开头的一个url，打开这个url，你会发现里面是很多的.eof文件和.woff文件，这里面其实就是css加密的转换对应表，在使用百度的在线字体可视化工具打开之后，可以看到里面的一个对应的情况的。在这里我自己的操作是先将这些.woff文件通过TFFont转化成.xml文件，然后再xml的parse定位构建字典，从而建立对应表（项目里称为密码表），**注意，这个对应表是会随时更新的，而且每次更新都会有几个文件，所以每次运行的时候要先查询这些文件有没有更新**，建立对应表之后，缓存，然后因为一次的运行的时候s3c那个链接里会有多个.woff文件，所以会有多个对应表，需要通过测试找到真正的是哪个对应表，再通过替换，将评论里面缺失的字进行替换就可以获取到实际的评论了。

对于这个真正的对应表，应该在s3c的url里或者是评论的url里应该还是有线索的，感兴趣的小伙伴可以看看。

文件结构

> 破解密码表.py : 构建对应表  
> 密码表缓存.py : 存储对应表  
> 主页.py : 进行评论获取  

**2. 然后是重头戏有后缀的这个评论的爬取方法**
例子的url的话（http://www.dianping.com/shop/1541380745/review_all/p2 ）这个。这里的加密规则和第一个是不同的，第一个的话是需要我们自己构建密码表，但是这里的话需要的是我们自己找出对应规则，然后在很大的一个表格中找到那个字的所在位置。具体的破解规律的话再上面给出的一些链接里面应该是有声明的，这个加密规则应该是没有怎么改变过。  
具体的过程如下：  
1. 首先是要获取一个css的链接，这里面的是一个总的表格，这个css的链接在评论页面的/html/head/link[4]/@href中可以找到  
2. 然后构建文字字典，其实这里也是和第一种的构建对应表一个样  
3. 再者就是进行替换，将svg替换成为相应的文字  

变量说明：
1. href为需要爬取的url（**记住是详情页面**）
2. page为页数，也是.csv文件的保存格式
3. cookie为上面href的网页cookie，不过只需要获取一次就行
4. main()函数里面的Cookie是http://www.dianping.com/guangzhou/ch10/r13880 这个链接的cookie，这里的做法是现在这个url登录上去再跳转到别的页面去，之前有试过登录不上

注意：
1. 编码格式的问题，因为最后是保存为.csv格式，所以最后的encoding是gbk格式，所以在保存的时候有可能出现报错，这里的报错都是因为编码格式的问题，具体是因为大众点评里面的表情，这里采用的方法是直接replace
2. 网页格式的问题，评论页面的每个< li>标签都是通过渲染出来的，所以用etree似乎不是很好，最好还是使用正则表达式比较好
3. 评论不全的问题，评论只能显示点击“查看全部”之前的前面一小段的评论，后面的评论得通过点击按钮之后才能看，但是这部分浏览器会通过发送并接受html头进行重新渲染的，所以这部分暂时不知道怎么操作

文件结构

> 详情.py：爬虫  
> 其他文件夹：存储.csv文件  
