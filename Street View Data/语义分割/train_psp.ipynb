{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Train PSPNet on ADE20K Dataset\n",
        "\n",
        "This is a tutorial of training PSPNet on ADE20K dataset using Gluon Vison.\n",
        "The readers should have basic knowledge of deep learning and should be familiar with Gluon API.\n",
        "New users may first go through `A 60-minute Gluon Crash Course <http://gluon-crash-course.mxnet.io/>`_.\n",
        "You can `Start Training Now`_ or `Dive into Deep`_.\n",
        "\n",
        "## Start Training Now\n",
        "\n",
        ".. hint::\n",
        "\n",
        "    Feel free to skip the tutorial because the training script is self-complete and ready to launch.\n",
        "\n",
        "    :download:`Download Full Python Script: train.py<../../../scripts/segmentation/train.py>`\n",
        "\n",
        "    Example training command::\n",
        "\n",
        "        CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset ade20k --model psp --backbone resnet50 --syncbn --epochs 120 --lr 0.01 --checkname mycheckpoint\n",
        "\n",
        "    For more training command options, please run ``python train.py -h``\n",
        "    Please checkout the `model_zoo <../model_zoo/index.html#semantic-segmentation>`_ for training commands of reproducing the pretrained model.\n",
        "\n",
        "## Dive into Deep\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mxnet as mx\n",
        "from mxnet import gluon, autograd\n",
        "import gluoncv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pyramid Scene Parsing Network\n",
        "\n",
        "<img src=\"https://hszhao.github.io/projects/pspnet/figures/pspnet.png\" width=\"80%\" align=\"center\">\n",
        "\n",
        "(figure credit to `Zhao et al. <https://arxiv.org/pdf/1612.01105.pdf>`_ )\n",
        "\n",
        "Pyramid Scene Parsing Network (PSPNet) [Zhao17]_  exploit the\n",
        "capability of global context information by different-regionbased\n",
        "context aggregation through the pyramid pooling module.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PSPNet Model\n",
        "\n",
        "A Pyramid Pooling Module is built on top of FCN, which combines multiple scale\n",
        "features with different receptive field sizes. It pools the featuremaps\n",
        "into different sizes and then concatenating together after upsampling.\n",
        "\n",
        "The Pyramid Pooling Module is defined as::\n",
        "\n",
        "    class _PyramidPooling(HybridBlock):\n",
        "        def __init__(self, in_channels, **kwargs):\n",
        "            super(_PyramidPooling, self).__init__()\n",
        "            out_channels = int(in_channels/4)\n",
        "            with self.name_scope():\n",
        "                self.conv1 = _PSP1x1Conv(in_channels, out_channels, **kwargs)\n",
        "                self.conv2 = _PSP1x1Conv(in_channels, out_channels, **kwargs)\n",
        "                self.conv3 = _PSP1x1Conv(in_channels, out_channels, **kwargs)\n",
        "                self.conv4 = _PSP1x1Conv(in_channels, out_channels, **kwargs)\n",
        "\n",
        "        def pool(self, F, x, size):\n",
        "            return F.contrib.AdaptiveAvgPooling2D(x, output_size=size)\n",
        "\n",
        "        def upsample(self, F, x, h, w):\n",
        "            return F.contrib.BilinearResize2D(x, height=h, width=w)\n",
        "\n",
        "        def hybrid_forward(self, F, x):\n",
        "            _, _, h, w = x.shape\n",
        "            feat1 = self.upsample(F, self.conv1(self.pool(F, x, 1)), h, w)\n",
        "            feat2 = self.upsample(F, self.conv2(self.pool(F, x, 2)), h, w)\n",
        "            feat3 = self.upsample(F, self.conv3(self.pool(F, x, 3)), h, w)\n",
        "            feat4 = self.upsample(F, self.conv4(self.pool(F, x, 4)), h, w)\n",
        "            return F.concat(x, feat1, feat2, feat3, feat4, dim=1)\n",
        "\n",
        "PSPNet model is provided in :class:`gluoncv.model_zoo.PSPNet`. To get\n",
        "PSP model using ResNet50 base network for ADE20K dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.crop_size 480\nPSPNet(\n  (conv1): HybridSequential(\n    (0): Conv2D(3 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n    (2): Activation(relu)\n    (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n    (5): Activation(relu)\n    (6): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  )\n  (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n  (relu): Activation(relu)\n  (maxpool): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n  (layer1): HybridSequential(\n    (0): BottleneckV1b(\n      (conv1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu3): Activation(relu)\n      (downsample): HybridSequential(\n        (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      )\n    )\n    (1): BottleneckV1b(\n      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu3): Activation(relu)\n    )\n    (2): BottleneckV1b(\n      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu3): Activation(relu)\n    )\n  )\n  (layer2): HybridSequential(\n    (0): BottleneckV1b(\n      (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu3): Activation(relu)\n      (downsample): HybridSequential(\n        (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      )\n    )\n    (1): BottleneckV1b(\n      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu3): Activation(relu)\n    )\n    (2): BottleneckV1b(\n      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu3): Activation(relu)\n    )\n    (3): BottleneckV1b(\n      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu3): Activation(relu)\n    )\n  )\n  (layer3): HybridSequential(\n    (0): BottleneckV1b(\n      (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n      (relu3): Activation(relu)\n      (downsample): HybridSequential(\n        (0): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n      )\n    )\n    (1): BottleneckV1b(\n      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n      (relu3): Activation(relu)\n    )\n    (2): BottleneckV1b(\n      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n      (relu3): Activation(relu)\n    )\n    (3): BottleneckV1b(\n      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n      (relu3): Activation(relu)\n    )\n    (4): BottleneckV1b(\n      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n      (relu3): Activation(relu)\n    )\n    (5): BottleneckV1b(\n      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n      (relu3): Activation(relu)\n    )\n  )\n  (layer4): HybridSequential(\n    (0): BottleneckV1b(\n      (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n      (relu3): Activation(relu)\n      (downsample): HybridSequential(\n        (0): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n      )\n    )\n    (1): BottleneckV1b(\n      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n      (relu3): Activation(relu)\n    )\n    (2): BottleneckV1b(\n      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu1): Activation(relu)\n      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (relu2): Activation(relu)\n      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n      (relu3): Activation(relu)\n    )\n  )\n  (head): _PSPHead(\n    (psp): _PyramidPooling(\n      (conv1): HybridSequential(\n        (0): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n        (2): Activation(relu)\n      )\n      (conv2): HybridSequential(\n        (0): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n        (2): Activation(relu)\n      )\n      (conv3): HybridSequential(\n        (0): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n        (2): Activation(relu)\n      )\n      (conv4): HybridSequential(\n        (0): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n        (2): Activation(relu)\n      )\n    )\n    (block): HybridSequential(\n      (0): Conv2D(4096 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n      (2): Activation(relu)\n      (3): Dropout(p = 0.1, axes=())\n      (4): Conv2D(512 -> 150, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (auxlayer): _FCNHead(\n    (block): HybridSequential(\n      (0): Conv2D(1024 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n      (2): Activation(relu)\n      (3): Dropout(p = 0.1, axes=())\n      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)\n"
          ]
        }
      ],
      "source": [
        "model = gluoncv.model_zoo.get_psp(dataset='ade20k', backbone='resnet50', pretrained=False)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset and Data Augmentation\n",
        "\n",
        "image transform for color normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mxnet.gluon.data.vision import transforms\n",
        "input_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We provide semantic segmentation datasets in :class:`gluoncv.data`.\n",
        "For example, we can easily get the ADE20K dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "/Users/creative/.mxnet/datasets/ade is not a valid dir. Did you forget to initialize                          datasets described in:                          `https://cv.gluon.ai/build/examples_datasets/index.html`?                          You need to initialize each dataset only once.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d3c4e9a2aca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluoncv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADE20KSegmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# set batch_size = 2 for toy example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create Training Loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.7/site-packages/gluoncv-0.10.0-py3.7.egg/gluoncv/data/ade20k/segmentation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, mode, transform, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     def __init__(self, root=os.path.expanduser('~/.mxnet/datasets/ade'),\n\u001b[1;32m    100\u001b[0m                  split='train', mode=None, transform=None, **kwargs):\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mADE20KSegmentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Please setup the dataset using\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.7/site-packages/gluoncv-0.10.0-py3.7.egg/gluoncv/data/segbase.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, mode, transform, base_size, crop_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# pylint: disable=abstract-method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m520\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSegmentationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.7/site-packages/gluoncv-0.10.0-py3.7.egg/gluoncv/data/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m     29\u001b[0m                          \u001b[0;31m`\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mexamples_datasets\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                          \u001b[0mYou\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mto\u001b[0m \u001b[0minitialize\u001b[0m \u001b[0meach\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhelper_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /Users/creative/.mxnet/datasets/ade is not a valid dir. Did you forget to initialize                          datasets described in:                          `https://cv.gluon.ai/build/examples_datasets/index.html`?                          You need to initialize each dataset only once."
          ]
        }
      ],
      "source": [
        "trainset = gluoncv.data.ADE20KSegmentation(split='train', transform=input_transform)\n",
        "print('Training images:', len(trainset))\n",
        "# set batch_size = 2 for toy example\n",
        "batch_size = 2\n",
        "# Create Training Loader\n",
        "train_data = gluon.data.DataLoader(\n",
        "    trainset, batch_size, shuffle=True, last_batch='rollover',\n",
        "    num_workers=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For data augmentation,\n",
        "we follow the standard data augmentation routine to transform the input image\n",
        "and the ground truth label map synchronously. (*Note that \"nearest\"\n",
        "mode upsample are applied to the label maps to avoid messing up the boundaries.*)\n",
        "We first randomly scale the input image from 0.5 to 2.0 times, then rotate\n",
        "the image from -10 to 10 degrees, and crop the image with padding if needed.\n",
        "Finally a random Gaussian blurring is applied.\n",
        "\n",
        "Random pick one example for visualization:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-38eb1745e5aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgluoncv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_color_pallete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeNormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainset' is not defined"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "random.seed(datetime.now())\n",
        "idx = random.randint(0, len(trainset))\n",
        "img, mask = trainset[idx]\n",
        "from gluoncv.utils.viz import get_color_pallete, DeNormalize\n",
        "# get color pallete for visualize mask\n",
        "mask = get_color_pallete(mask.asnumpy(), dataset='ade20k')\n",
        "mask.save('mask.png')\n",
        "# denormalize the image\n",
        "img = DeNormalize([.485, .456, .406], [.229, .224, .225])(img)\n",
        "img = np.transpose((img.asnumpy()*255).astype(np.uint8), (1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the image and mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "# subplot 1 for img\n",
        "fig = plt.figure()\n",
        "fig.add_subplot(1,2,1)\n",
        "\n",
        "plt.imshow(img)\n",
        "# subplot 2 for the mask\n",
        "mmask = mpimg.imread('mask.png')\n",
        "fig.add_subplot(1,2,2)\n",
        "plt.imshow(mmask)\n",
        "# display\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Details\n",
        "\n",
        "- Training Losses:\n",
        "\n",
        "    We apply a standard per-pixel Softmax Cross Entropy Loss to train PSPNet. \n",
        "    Additionally, an Auxiliary Loss as in PSPNet [Zhao17]_ at Stage 3 can be enabled when\n",
        "    training with command ``--aux``. This will create an additional FCN \"head\" after Stage 3.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv.loss import MixSoftmaxCrossEntropyLoss\n",
        "criterion = MixSoftmaxCrossEntropyLoss(aux=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Learning Rate and Scheduling:\n",
        "\n",
        "    We use different learning rate for PSP \"head\" and the base network. For the PSP \"head\",\n",
        "    we use $10\\times$ base learning rate, because those layers are learned from scratch.\n",
        "    We use a poly-like learning rate scheduler for FCN training, provided in :class:`gluoncv.utils.LRScheduler`.\n",
        "    The learning rate is given by $lr = base_lr \\times (1-iter)^{power}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr_scheduler = gluoncv.utils.LRScheduler(mode='poly', base_lr=0.001,\n",
        "                                         nepochs=50, iters_per_epoch=len(train_data), power=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Dataparallel for multi-gpu training, using cpu for demo only\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv.utils.parallel import *\n",
        "ctx_list = [mx.cpu(0)]\n",
        "model = DataParallelModel(model, ctx_list)\n",
        "criterion = DataParallelCriterion(criterion, ctx_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create SGD solver\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kv = mx.kv.create('local')\n",
        "optimizer = gluon.Trainer(model.module.collect_params(), 'sgd',\n",
        "                          {'lr_scheduler': lr_scheduler,\n",
        "                           'wd':0.0001,\n",
        "                           'momentum': 0.9,\n",
        "                           'multi_precision': True},\n",
        "                          kvstore = kv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The training loop\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_loss = 0.0\n",
        "epoch = 0\n",
        "for i, (data, target) in enumerate(train_data):\n",
        "    with autograd.record(True):\n",
        "        outputs = model(data)\n",
        "        losses = criterion(outputs, target)\n",
        "        mx.nd.waitall()\n",
        "        autograd.backward(losses)\n",
        "    optimizer.step(batch_size)\n",
        "    for loss in losses:\n",
        "        train_loss += loss.asnumpy()[0] / len(losses)\n",
        "    print('Epoch %d, batch %d, training loss %.3f'%(epoch, i, train_loss/(i+1)))\n",
        "    # just demo for 2 iters\n",
        "    if i > 1:\n",
        "        print('Terminated for this demo...')\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can `Start Training Now`_.\n",
        "\n",
        "### References\n",
        "\n",
        ".. [Long15] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. \\\n",
        "    \"Fully convolutional networks for semantic segmentation.\" \\\n",
        "    Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n",
        "\n",
        ".. [Zhao17] Zhao, Hengshuang, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. \\\n",
        "    \"Pyramid scene parsing network.\" IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2017.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}